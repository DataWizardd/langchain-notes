{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4355356b-a3ae-4fdf-8a67-ab2caf68f29d",
   "metadata": {},
   "source": [
    "# LangChain ì‹œì‘í•˜ê¸°\n",
    "\n",
    "## 01. ë­ì²´ì¸(LangChain) ê°œìš”\n",
    "\n",
    "### ğŸ“Œ ê°œìš”\n",
    "- **LangChain** = LLMì„ ì‹¤ìš©ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬\n",
    "- ëª©ì :\n",
    "  1. **LLM + ì™¸ë¶€ ë°ì´í„°** (DB, ë¬¸ì„œ, API ë“±) ì—°ê²°\n",
    "  2. **LLM + ë„êµ¬** (ê²€ìƒ‰, ê³„ì‚°ê¸°, ì½”ë“œ ì‹¤í–‰) ì—°ê²°\n",
    "  3. **LLM ì›Œí¬í”Œë¡œìš°** ê´€ë¦¬ ë° ì²´ì¸í™”\n",
    "\n",
    "### âš™ï¸ ì£¼ìš” êµ¬ì„± ìš”ì†Œ\n",
    "| êµ¬ì„± ìš”ì†Œ | ì„¤ëª… |\n",
    "|-----------|------|\n",
    "| **PromptTemplate** | í”„ë¡¬í”„íŠ¸ë¥¼ í…œí”Œë¦¿í™”í•˜ì—¬ ì¬ì‚¬ìš© ê°€ëŠ¥ |\n",
    "| **LLM** | GPT, Claude, LLaMA ë“± ë‹¤ì–‘í•œ ëª¨ë¸ ì—°ê²° |\n",
    "| **Chains** | ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ë¬¶ì–´ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ |\n",
    "| **Agents** | LLMì´ ë„êµ¬(ê²€ìƒ‰, ê³„ì‚°ê¸°, DB ë“±)ë¥¼ ì„ íƒÂ·ì‹¤í–‰ |\n",
    "| **Memory** | ëŒ€í™” ê¸°ë¡ ë° ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 02. LangChain\n",
    "\n",
    "### ğŸ“Œ ê°œë…\n",
    "- LangChainì€ **OpenAI, Anthropic, HuggingFace** ë“± ë‹¤ì–‘í•œ LLM ì—°ê²° ì§€ì›\n",
    "- `ChatOpenAI` í´ë˜ìŠ¤ë¡œ ëŒ€í™”í˜• ëª¨ë¸ê³¼ ì§ì ‘ í†µì‹ \n",
    "- **temperature**: ì°½ì˜ì„± ì¡°ì ˆ (0 = ì •í™•ì„±â†‘, 1 = ë‹¤ì–‘ì„±â†‘)  \n",
    "- **model**: ëª¨ë¸ ì¢…ë¥˜ ì„ íƒ (ì˜ˆ: `gpt-3.5-turbo`, `gpt-4o-mini`)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c042523b-c6f7-4bb4-9621-e24deb304bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-73\n"
     ]
    }
   ],
   "source": [
    "### ğŸ’» ì˜ˆì‹œ ì½”ë“œ\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(api_key[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08819775-7b3d-4e37-b599-69421a6ed88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainì€ ì–¸ì–´ ê°„ ìƒí˜¸ì‘ìš©ì„ ì´‰ì§„í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ë‹¤ì–‘í•œ ì–¸ì–´ê°„ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì„ ìš©ì´í•˜ê²Œ í•˜ê³ , ì–´íœ˜, ë¬¸ë²•, ë¬¸í™” ì°¨ì´ë¥¼ ê·¹ë³µí•˜ëŠ”ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. LangChainì€ ì „ ì„¸ê³„ ì‚¬ëŒë“¤ì´ ì„œë¡œ ì†Œí†µí•˜ê³  í˜‘ë ¥í•  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ì œê³µí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ì§ˆë¬¸ ë³´ë‚´ê¸°\n",
    "response = llm.invoke(\"LangChainì´ ë¬´ì—‡ì¸ì§€ ê°„ë‹¨íˆ ì„¤ëª…í•´ì¤˜\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef825ef-8474-4f92-b9ff-147be162e78a",
   "metadata": {},
   "source": [
    "## 03. í† í°(Token), í† í°ê³„ì‚°ê¸°, ëª¨ë¸ë³„ ë¹„ìš©\n",
    "\n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- í† í°(Token) = ì–¸ì–´ëª¨ë¸ì´ ì²˜ë¦¬í•˜ëŠ” ìµœì†Œ ë‹¨ìœ„ (sub-word, ìŒì ˆ ë‹¨ìœ„)\n",
    "- í† í° ìˆ˜ = ì…ë ¥ + ì¶œë ¥ í•©ì‚° (â†’ ë¹„ìš©ê³¼ ì†ë„ì— ì˜í–¥)\n",
    "- Context Window = ëª¨ë¸ì´ í•œ ë²ˆì— ì²˜ë¦¬ ê°€ëŠ¥í•œ ìµœëŒ€ í† í° ìˆ˜\n",
    "- ë¹„ìš© = ëª¨ë¸ë³„ 1K tokens ê¸°ì¤€ ìš”ê¸ˆ ì¡´ì¬ (ì˜ˆ: GPT-3.5 = $0.0015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a5cbc-ea60-4464-b650-c88513a152ae",
   "metadata": {},
   "source": [
    "## 04. ChatOpenAI ì£¼ìš” íŒŒë¼ë¯¸í„°, invoke(), stream() \n",
    "\n",
    "### ğŸ“Œ ì£¼ìš” íŒŒë¼ë¯¸í„°\n",
    "- **model** : ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ì˜ˆ: `gpt-3.5-turbo`, `gpt-4o`)\n",
    "- **temperature** : ì°½ì˜ì„± ì¡°ì ˆ (0 â†’ ì‚¬ì‹¤ ìœ„ì£¼, 1 â†’ ì°½ì˜ì )\n",
    "- **max_tokens** : ì¶œë ¥ ìµœëŒ€ í† í° ìˆ˜\n",
    "- **top_p, top_k** : ìƒ˜í”Œë§ ë°©ì‹ ì¡°ì ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e5bbe48-13b0-4172-9a71-eb3541d9f23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainì€ ì–¸ì–´ ë²ˆì—­ ë° í†µì—­ì„ ì œê³µí•˜ëŠ” í”Œë«í¼ìœ¼ë¡œ, ì‚¬ìš©ìë“¤ì€ ë‹¤ì–‘í•œ ì–¸ì–´ ê°„ì˜ ì†Œí†µì„ ì‰½ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ë³´ì•ˆì„±ê³¼ ì‹ ë¢°ì„±ì„ ë†’ì´ë©°, ì§€ì†ì ì¸ ì–¸ì–´ í•™ìŠµê³¼ ë°œì „ì„ ì§€ì›í•©ë‹ˆë‹¤. ë˜í•œ ì‚¬ìš©ìë“¤ì€ LangChain ë‚´ì—ì„œ ë‹¤ì–‘í•œ ì–¸ì–´ ê´€ë ¨ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "----------------------------------------------\n",
      "ì•Œê² ìŠµë‹ˆë‹¤! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
     ]
    }
   ],
   "source": [
    "### ğŸ’» ì˜ˆì‹œ ì½”ë“œ\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# invoke() â†’ í•œë²ˆì— ì‘ë‹µ ìƒì„±\n",
    "response = llm.invoke(\"LangChainì˜ í•µì‹¬ ê¸°ëŠ¥ì„ 3ì¤„ë¡œ ìš”ì•½í•´ì¤˜.\")\n",
    "print(response.content)\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "# stream() â†’ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ\n",
    "for chunk in llm.stream(\"ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•´ì¤˜.\"):\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5510210b-1a52-4680-b15b-a68b36872456",
   "metadata": {},
   "source": [
    "## 05. LangSmith ë¡œ GPT ì¶”ë¡ ë‚´ìš© ì¶”ì \n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- LangSmith = LangChain ê³µì‹ ì‹¤í—˜/ê´€ì°° í”Œë«í¼\n",
    "- LLM ì¶”ë¡  ê³¼ì •(ì…ë ¥, ì¶œë ¥, ì²´ì¸ ì‹¤í–‰ ë¡œê·¸) ê¸°ë¡ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62a1169b-786e-4ad9-8d29-6023a4c25931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, the weather is really nice today.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "prompt = PromptTemplate.from_template(\"ë‹¤ìŒ ë¬¸ì¥ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì¤˜: {sentence}\")\n",
    "\n",
    "# ì²´ì¸ ìƒì„± (LCEL)\n",
    "chain = prompt | llm\n",
    "\n",
    "# ì‹¤í–‰ (LangSmith ëŒ€ì‹œë³´ë“œì— ìë™ ê¸°ë¡ë¨)\n",
    "result = chain.invoke({\"sentence\": \"ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì°¸ ì¢‹ë„¤ìš”.\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa654b9e-44a1-4b91-ab4c-fbc8bbf52c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's go watch a movie tonight.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "parser_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "result = parser_chain.invoke({\"sentence\": \"ì˜¤ëŠ˜ ì €ë…ì— ì˜í™” ë³´ëŸ¬ ê°€ì.\"})\n",
    "print(result)  # ë¬¸ìì—´ë§Œ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1400130e-5beb-478f-8414-03040ef1c712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english': 'I drank coffee this morning.', 'french': 'Je ai bu du cafÃ© ce matin.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "multi_chain = RunnableParallel(\n",
    "    english=(PromptTemplate.from_template(\"Translate to English: {text}\") | llm | StrOutputParser()),\n",
    "    french=(PromptTemplate.from_template(\"Translate to French: {text}\") | llm | StrOutputParser())\n",
    ")\n",
    "\n",
    "result = multi_chain.invoke({\"text\": \"ë‚˜ëŠ” ì˜¤ëŠ˜ ì•„ì¹¨ì— ì»¤í”¼ë¥¼ ë§ˆì…¨ë‹¤.\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce552c-7be3-424c-84ae-f0cdba9081cb",
   "metadata": {},
   "source": [
    "## 06. GPT-4o (ë©€í‹°ëª¨ë‹¬) ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ì¸ì‹í•˜ì—¬ ë‹µë³€ ì¶œë ¥\n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- GPT-4o = OpenAI ë©€í‹°ëª¨ë‹¬ ëª¨ë¸\n",
    "- í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ì…ë ¥ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7176838-b9d3-4581-8a69-4d7effa6cae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ë¯¸ì§€ì—ëŠ” \"MOTEL\"ì´ë¼ëŠ” ê¸€ìê°€ ì“°ì¸ ê°„íŒì´ ë³´ì…ë‹ˆë‹¤. ê°„íŒì€ ê¸°ë‘¥ì— ì„¸ì›Œì ¸ ìˆê³  í•˜ëŠ˜ì„ ë°°ê²½ìœ¼ë¡œ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìƒë‹¨ì—ëŠ” ì•ˆí…Œë‚˜ ê°™ì€ êµ¬ì¡°ë¬¼ì´ ë³´ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"ì´ ì´ë¯¸ì§€ì—ëŠ” ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?\"},\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": \"https://picsum.photos/200\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = llm.invoke([message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e554c-c986-4a97-84ee-2b8f8414dd62",
   "metadata": {},
   "source": [
    "## 07. LCEL ë¡œ Chain ìƒì„± \n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- LCEL (LangChain Expression Language)\n",
    "- ì²´ì¸ì„ ì§ê´€ì ìœ¼ë¡œ | ì—°ì‚°ìë¡œ ì—°ê²° ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d77bf354-66e1-4058-8940-52fba53e151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¢‹ì€ ì•„ì¹¨ (joh-eun achim)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Translate the following into Korean: {text}\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"text\": \"Good morning\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41af74d0-04c2-4934-b22e-0501e1bbe7a3",
   "metadata": {},
   "source": [
    "- ì—¬ëŸ¬ ì…ë ¥ì„ ë™ì‹œì— ì²˜ë¦¬ (batch inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a72dfd1f-67b4-48e7-9ba8-2ed3c9e2a867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì•ˆë…•í•˜ì„¸ìš”', 'ê°ì‚¬í•©ë‹ˆë‹¤ (gam-sa-ham-ni-da)']\n"
     ]
    }
   ],
   "source": [
    "inputs = [{\"text\": \"Hello\"}, {\"text\": \"Thank you\"}]\n",
    "results = chain.batch(inputs)\n",
    "print([r for r in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c347a8-6639-4043-9ba2-a8a3292953be",
   "metadata": {},
   "source": [
    "## 08. ì¶œë ¥íŒŒì„œ(StrOutputParser) ë¥¼ ì²´ì¸ì— ì—°ê²° \n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- ëª¨ë¸ ì¶œë ¥ â†’ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "- ì—¬ëŸ¬ í›„ì²˜ë¦¬ íŒŒì„œ ì œê³µ (JSON, List ë“±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8df5866-3f66-4fc0-bc49-d040f318c548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°˜ê°€ì›Œìš”\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke({\"text\": \"Nice to meet you\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88850a1-c464-4a8d-af8b-43d5cc4ff29e",
   "metadata": {},
   "source": [
    "## 09. ë¹„ë™ê¸°(asynchronous) í˜¸ì¶œ ë°©ë²• \n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- asyncio ê¸°ë°˜ ë¹„ë™ê¸° ì‹¤í–‰\n",
    "- ë™ì‹œì— ì—¬ëŸ¬ ìš”ì²­ ì²˜ë¦¬ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "147dd3b3-1a4f-415f-857a-f475f580159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "\n",
    "# async def run_async():\n",
    "#     result = await chain.ainvoke({\"text\": \"Good night\"})\n",
    "#     print(result)\n",
    "\n",
    "# asyncio.run(run_async())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e6feebd-d720-4c3a-bbe1-aabe49c800c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜ ìì„¸ìš” (jal jaseyo)\n"
     ]
    }
   ],
   "source": [
    "result = await chain.ainvoke({\"text\": \"Good night\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b72e7dc-ef26-4ff7-8d58-7c9ed56951b2",
   "metadata": {},
   "source": [
    "## 10. ë³‘ë ¬ì²´ì¸ êµ¬ì„±(RunnableParallel)\n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- ì—¬ëŸ¬ ì²´ì¸ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰ í›„ ê²°ê³¼ í•©ì¹˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a34c216-6b0e-433a-94d5-4c12cda3c9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'korean': '\"How are you?\" in Korean is \"ì˜ ì§€ë‚´ì„¸ìš”?\" (jal jinaeseyo?).', 'english': '\"How are you?\" can be translated into Korean as \"ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?\"'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "parallel_chain = RunnableParallel(\n",
    "    korean=prompt | llm | StrOutputParser(),\n",
    "    english=prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "results = parallel_chain.invoke({\"text\": \"How are you?\"})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd3ec61-f17a-4ce7-bda8-68b532e2710e",
   "metadata": {},
   "source": [
    "## 11. ê°’ì„ ì „ë‹¬í•´ì£¼ëŠ” RunnablePassthrough\n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ë‹¤ìŒ ì²´ì¸ì— ì „ë‹¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d84a18f0-a126-42f3-9f57-331e7fe0dd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¼ë¡ ì…ë‹ˆë‹¤! ì›í•˜ì‹œëŠ” ë¬¸ì¥ì„ ë§ì”€í•´ì£¼ì‹œë©´ ê·¸ëŒ€ë¡œ ì „ë‹¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "passthrough_chain = RunnablePassthrough() | llm\n",
    "response = passthrough_chain.invoke(\"ê·¸ëƒ¥ ì´ ë¬¸ì¥ì„ ëª¨ë¸ë¡œ ë³´ë‚´ì¤˜.\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7010ab-a2e4-4078-b76f-b12337fc617e",
   "metadata": {},
   "source": [
    "## 12. ë³‘ë ¬ë¡œ Runnable ì„ ì‹¤í–‰í•˜ëŠ” RunnableParallel\n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- ì—¬ëŸ¬ Runnableì„ ë™ì‹œì— ì‹¤í–‰\n",
    "- ê²°ê³¼ë¥¼ dict í˜•íƒœë¡œ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f088f3-4fdb-4263-8dae-7a774f17f15a",
   "metadata": {},
   "source": [
    "## 13. í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ëŠ” RunnableLambda, itemgetter\n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- RunnableLambda : Python í•¨ìˆ˜ ì‹¤í–‰ì„ Runnableë¡œ ë˜í•‘\n",
    "- itemgetter : ì…ë ¥ dictì—ì„œ íŠ¹ì • keyë§Œ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7380924d-0211-47fc-8a8a-ef27d0113ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_80956533cb', 'id': 'chatcmpl-C9aMg2lxJptqDHcmFXXHAb5Hf1gwK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--df4b1473-ed8e-4b5e-8e3c-b1548cc6b6cc-0' usage_metadata={'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "I'm glad to hear that you're enjoying LangChain! It's a popular framework for building applications with large language models. If you have any questions or need more information about how to make the most out of LangChain, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "from operator import itemgetter\n",
    "\n",
    "def to_upper(x):\n",
    "    return x.upper()\n",
    "\n",
    "lambda_chain = RunnableLambda(to_upper) | llm\n",
    "print(lambda_chain.invoke(\"hello\"))\n",
    "\n",
    "# itemgetter ì˜ˆì‹œ\n",
    "getter_chain = itemgetter(\"text\") | llm\n",
    "print(getter_chain.invoke({\"text\": \"LangChain is fun\"}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988e406c-06a4-4c18-b6b6-413d3eae1088",
   "metadata": {},
   "source": [
    "## âœ… ì •ë¦¬\n",
    "\n",
    "- ChatOpenAI ì£¼ìš” ë©”ì„œë“œ: invoke(), stream(), batch(), ainvoke()\n",
    "\n",
    "- LCEL ë¬¸ë²•ìœ¼ë¡œ ì²´ì¸ì„ ì§ê´€ì ìœ¼ë¡œ êµ¬ì„± ê°€ëŠ¥ (|)\n",
    "\n",
    "- LangSmithì—ì„œ ì²´ì¸ ì‹¤í–‰ê³¼ì •ì„ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥\n",
    "\n",
    "- Runnable ê³„ì—´ (RunnableParallel, RunnablePassthrough, RunnableLambda)ë¡œ ìœ ì—°í•œ ì²´ì¸ ì„¤ê³„ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e60044-cc02-4dac-85d2-ea168a9b0fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_new_env)",
   "language": "python",
   "name": "my_new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
