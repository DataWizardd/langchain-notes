{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "251dc0ca-be14-4cf6-a412-328dd45e9e4f",
   "metadata": {},
   "source": [
    "### ğŸ“Œ LangGraph ê°œìš”\n",
    "- **LangChain ê¸°ë°˜ì˜ Workflow Orchestration ë¼ì´ë¸ŒëŸ¬ë¦¬**\n",
    "- LLM + Tool + Memory + ì¡°ê±´ë¶€ ë¶„ê¸° ë“±ì„ **ê·¸ë˜í”„ êµ¬ì¡°**ë¡œ í‘œí˜„\n",
    "- í•µì‹¬ ê°œë…:\n",
    "  - **State**: ë…¸ë“œ ê°„ ê³µìœ ë˜ëŠ” ìƒíƒœ (TypedDict ë“±ìœ¼ë¡œ ì •ì˜)\n",
    "  - **Node**: ìƒíƒœë¥¼ ì…ë ¥ë°›ì•„ ê°€ê³µí•˜ëŠ” í•¨ìˆ˜/LLM í˜¸ì¶œ/Tool\n",
    "  - **Edge**: ë…¸ë“œ ê°„ ì—°ê²° (ì¡°ê±´ë¶€ ë¶„ê¸° ê°€ëŠ¥)\n",
    "  - **Compile**: ê·¸ë˜í”„ë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°ì²´ë¡œ ë³€í™˜\n",
    "- ì¥ì :\n",
    "  - ì²´ì¸ë³´ë‹¤ **ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°**ë¥¼ ìœ ì—°í•˜ê²Œ êµ¬ì„±\n",
    "  - ë””ë²„ê¹…/ì¬ì‹¤í–‰/ë¶„ê¸° ì œì–´ê°€ ìš©ì´\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cdd3b14-427e-4b51-b413-4704d4eda8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ì„¤ì¹˜: pip install langgraph\n",
    "\n",
    "# from langgraph.graph import StateGraph, END\n",
    "\n",
    "# # ê°„ë‹¨í•œ Hello World ê·¸ë˜í”„ ì˜ˆì‹œ\n",
    "# def hello_node(state):\n",
    "#     print(\"Hello, LangGraph!\")\n",
    "#     return state\n",
    "\n",
    "# # ê·¸ë˜í”„ ì •ì˜\n",
    "# graph = StateGraph(dict)\n",
    "# graph.add_node(\"hello\", hello_node)\n",
    "# graph.set_entry_point(\"hello\")\n",
    "# graph.set_finish_point(\"hello\")\n",
    "\n",
    "# app = graph.compile()\n",
    "# app.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b51f8-eb64-4dc9-934c-38f10a8bcba2",
   "metadata": {},
   "source": [
    "### ğŸ“Œ LangGraph ê¸°ë³¸ êµ¬ì¡°\n",
    "- **State**: TypedDictë¡œ ëª…ì‹œì  íƒ€ì… ì§€ì •\n",
    "- **Node**: ìƒíƒœë¥¼ ë°›ì•„ì„œ ê°€ê³µ í›„ ë°˜í™˜\n",
    "- **Edge**: ë‹¨ìˆœ ì—°ê²° or ì¡°ê±´ë¶€ ì—°ê²°\n",
    "- **Compile**: `graph.compile()` í›„ `.invoke()`ë¡œ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18535cf2-8b1c-48dd-b175-a98fb3dce1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'LangGraphë€?', 'answer': 'ë‹µë³€: LangGraphë€?ì— ëŒ€í•œ ì˜ˆì‹œ ì‘ë‹µ'}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "# ìƒíƒœ ì •ì˜\n",
    "class MyState(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "# ë…¸ë“œ í•¨ìˆ˜\n",
    "def answer_node(state: MyState):\n",
    "    state[\"answer\"] = f\"ë‹µë³€: {state['question']}ì— ëŒ€í•œ ì˜ˆì‹œ ì‘ë‹µ\"\n",
    "    return state\n",
    "\n",
    "# ê·¸ë˜í”„ ë¹Œë“œ\n",
    "graph = StateGraph(MyState)\n",
    "graph.add_node(\"answer\", answer_node)\n",
    "graph.set_entry_point(\"answer\")\n",
    "graph.set_finish_point(\"answer\")\n",
    "\n",
    "app = graph.compile()\n",
    "print(app.invoke({\"question\": \"LangGraphë€?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27747845-a7da-4e62-9df1-db650cc7b65b",
   "metadata": {},
   "source": [
    "### ğŸ“Œ ìì£¼ ì“°ëŠ” ë¬¸ë²•\n",
    "- **TypedDict**: ìƒíƒœ íƒ€ì… ì •ì˜\n",
    "- **Annotated**: ìƒíƒœ ì—…ë°ì´íŠ¸ ë°©ì‹ ì§€ì • (append ë“±)\n",
    "- **Reducer**: ì—¬ëŸ¬ ë…¸ë“œ ê²°ê³¼ë¥¼ ëˆ„ì  ê´€ë¦¬\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9712c519-0e53-4899-b0ac-437d7eaf4090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': ['Hello!']}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# Reducer: ìƒíƒœ ì—…ë°ì´íŠ¸ ë°©ë²• ì •ì˜\n",
    "class State(TypedDict):\n",
    "    history: Annotated[list[str], \"append\"]\n",
    "\n",
    "def add_message(state: State):\n",
    "    state[\"history\"].append(\"Hello!\")\n",
    "    return state\n",
    "\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"add\", add_message)\n",
    "graph.set_entry_point(\"add\")\n",
    "graph.set_finish_point(\"add\")\n",
    "\n",
    "app = graph.compile()\n",
    "print(app.invoke({\"history\": []}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d792b3a8-f03c-4c57-95ef-3f55e3d90cc3",
   "metadata": {},
   "source": [
    "### ğŸ“Œ ì±—ë´‡ êµ¬í˜„\n",
    "- ìƒíƒœ = `messages` (ëŒ€í™” ì´ë ¥)\n",
    "- ë…¸ë“œ = LLM í˜¸ì¶œ\n",
    "- ì‹¤í–‰ = HumanMessage ì…ë ¥ â†’ AIMessage ì‘ë‹µ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "648cfa1a-aba2-4085-bd23-187d8b1d35b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64b4ebc2-68f9-4a2a-bc59-9b9f5a6b48c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='ì•ˆë…• LangGraph?', additional_kwargs={}, response_metadata={}), AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 14, 'total_tokens': 35, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C9w8JsXb3su4KODfLMrRtun6kwv84', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--11490e46-0296-43bc-97c7-4244ed7c4c4d-0', usage_metadata={'input_tokens': 14, 'output_tokens': 21, 'total_tokens': 35, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    messages: list\n",
    "\n",
    "def llm_node(state: ChatState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    state[\"messages\"].append(response)\n",
    "    return state\n",
    "\n",
    "graph = StateGraph(ChatState)\n",
    "graph.add_node(\"chat\", llm_node)\n",
    "graph.set_entry_point(\"chat\")\n",
    "graph.set_finish_point(\"chat\")\n",
    "\n",
    "app = graph.compile()\n",
    "print(app.invoke({\"messages\": [HumanMessage(content=\"ì•ˆë…• LangGraph?\")]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a0ede2-06e2-4fa7-9f0e-9c00fb0b0564",
   "metadata": {},
   "source": [
    "### ğŸ“Œ Function Calling + Conditional Edge\n",
    "- LLMì˜ í•¨ìˆ˜ í˜¸ì¶œ(Function Calling)ê³¼ ë„êµ¬ í˜¸ì¶œì„ ë…¸ë“œë¡œ êµ¬ì„±\n",
    "- ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ë…¸ë“œë¡œ ì´ë™ (if-else êµ¬ì¡°ë¥¼ Edgeë¡œ í‘œí˜„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0182df1-0793-4329-94c5-334ebe803d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?', 'answer': 'ì˜¤ëŠ˜ì€ ë§‘ìŠµë‹ˆë‹¤.'}\n",
      "{'question': 'LangGraphê°€ ë­ì•¼?', 'answer': 'ì¼ë°˜ì ì¸ ì§ˆë¬¸ ë‹µë³€ì…ë‹ˆë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "# pip install langgraph\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# 1) ìƒíƒœ ì •ì˜\n",
    "class MyState(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "# 2) ë¼ìš°íŒ… í•¨ìˆ˜ (ë¶„ê¸° ê²°ì •)\n",
    "def decide_route(state: MyState) -> str:\n",
    "    return \"weather\" if \"ë‚ ì”¨\" in state[\"question\"] else \"general\"\n",
    "\n",
    "# 3) ì‘ì—… ë…¸ë“œ\n",
    "def weather_node(state: MyState):\n",
    "    state[\"answer\"] = \"ì˜¤ëŠ˜ì€ ë§‘ìŠµë‹ˆë‹¤.\"\n",
    "    return state\n",
    "\n",
    "def general_node(state: MyState):\n",
    "    state[\"answer\"] = \"ì¼ë°˜ì ì¸ ì§ˆë¬¸ ë‹µë³€ì…ë‹ˆë‹¤.\"\n",
    "    return state\n",
    "\n",
    "# 4) ê·¸ë˜í”„ êµ¬ì„±\n",
    "graph = StateGraph(MyState)\n",
    "\n",
    "# (1) ë¼ìš°í„° ë…¸ë“œ: ìƒíƒœë§Œ í†µê³¼ì‹œí‚¤ëŠ” no-op\n",
    "def router(state: MyState): \n",
    "    return state\n",
    "\n",
    "graph.add_node(\"router\", router)\n",
    "graph.add_node(\"weather\", weather_node)\n",
    "graph.add_node(\"general\", general_node)\n",
    "\n",
    "# (2) ë¼ìš°í„°ì—ì„œ ì¡°ê±´ë¶€ ë¶„ê¸°\n",
    "graph.add_conditional_edges(\n",
    "    \"router\",\n",
    "    decide_route,                 # ë¶„ê¸° í•¨ìˆ˜: \"weather\" ë˜ëŠ” \"general\" ë°˜í™˜\n",
    "    {\"weather\": \"weather\", \"general\": \"general\"}  # ë°˜í™˜ê°’ â†’ ë…¸ë“œ ë§¤í•‘\n",
    ")\n",
    "\n",
    "# (3) ì¢…ë£Œ ì—£ì§€\n",
    "graph.add_edge(\"weather\", END)\n",
    "graph.add_edge(\"general\", END)\n",
    "\n",
    "# (4) ì§„ì… ì§€ì \n",
    "graph.set_entry_point(\"router\")\n",
    "\n",
    "# 5) ì»´íŒŒì¼ & ì‹¤í–‰\n",
    "app = graph.compile()\n",
    "print(app.invoke({\"question\": \"ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?\"}))\n",
    "print(app.invoke({\"question\": \"LangGraphê°€ ë­ì•¼?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd0dbd-efa6-41db-9f74-ab6065aea859",
   "metadata": {},
   "source": [
    "### ğŸ“Œ Checkpointer (ë©”ëª¨ë¦¬)\n",
    "- LangGraphëŠ” **Checkpointer**ë¡œ ì‹¤í–‰ ìƒíƒœ ì €ì¥\n",
    "- `thread_id` ë³„ë¡œ ëŒ€í™” ë§¥ë½ ìœ ì§€ ê°€ëŠ¥\n",
    "- MemorySaver = ê¸°ë³¸ ë©”ëª¨ë¦¬í˜• ì €ì¥ì†Œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b6efaa5-d219-4d40-a440-ca46699fa1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ì°¨ ì‘ë‹µ: ì €ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ë©°, ì´ë¦„ì€ ì—†ìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
      "2ì°¨ ì‘ë‹µ: ë‹¹ì‹ ì€ \"ë„ˆ ì´ë¦„ì´ ë­ì•¼?\"ë¼ê³  ë¬¼ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# pip install langgraph langchain-openai\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 1) ìƒíƒœì— 'messages'ë¥¼ ì •ì˜í•˜ê³ , ëˆ„ì  ë°©ì‹ì„ add_messagesë¡œ ì§€ì •\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# 2) LLM ë…¸ë“œ: ì „ì²´ messagesë¥¼ ë„£ì–´ ë‹µë³€ ë°›ê³ , ìƒˆ ë©”ì‹œì§€ë¡œ ë°˜í™˜ (ë¦¬ë“€ì„œê°€ ìë™ append)\n",
    "def chat_node(state: ChatState):\n",
    "    ai_msg = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [ai_msg]}  # â† ë°˜í™˜ì„ ë¦¬ìŠ¤íŠ¸ë¡œ! (ë¦¬ë“€ì„œê°€ ê¸°ì¡´ì— appendí•¨)\n",
    "\n",
    "# 3) ê·¸ë˜í”„ êµ¬ì„±\n",
    "graph = StateGraph(ChatState)\n",
    "graph.add_node(\"chat\", chat_node)\n",
    "graph.add_edge(START, \"chat\")\n",
    "graph.add_edge(\"chat\", END)\n",
    "\n",
    "# 4) ì²´í¬í¬ì¸í„° ì—°ê²°\n",
    "checkpointer = MemorySaver()\n",
    "app = graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "# 5) ë™ì¼ thread_idë¡œ ë‘ ë²ˆ í˜¸ì¶œ â†’ ë‘ ë²ˆì§¸ í˜¸ì¶œì—ì„œ ê³¼ê±° ëŒ€í™”ê°€ ìë™ ì£¼ì…\n",
    "cfg = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "r1 = app.invoke({\"messages\": [HumanMessage(content=\"ë„ˆ ì´ë¦„ì´ ë­ì•¼?\")]}, config=cfg)\n",
    "print(\"1ì°¨ ì‘ë‹µ:\", r1[\"messages\"][-1].content)\n",
    "\n",
    "r2 = app.invoke({\"messages\": [HumanMessage(content=\"ë‚´ê°€ ë°©ê¸ˆ ë­ë¼ê³  í–ˆì§€?\")]}, config=cfg)\n",
    "print(\"2ì°¨ ì‘ë‹µ:\", r2[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e70ba-c005-45e4-9984-9ce7433c004e",
   "metadata": {},
   "source": [
    "### ğŸ“Œ Stream ëª¨ë“œ & Interrupt\n",
    "- `.stream()` â†’ ë…¸ë“œë³„ ì¤‘ê°„ ê²°ê³¼ í™•ì¸\n",
    "- **Interrupt** â†’ íŠ¹ì • ì§€ì ì—ì„œ ì‹¤í–‰ ì¤‘ë‹¨, ì‚¬ëŒì´ ê°œì…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67ae6a8d-88e7-4dc9-bbcf-38060c694e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¤‘ê°„ ì—…ë°ì´íŠ¸: {'chat': {'messages': [AIMessage(content='LangGraphëŠ” ì–¸ì–´ ê°„ ìƒí˜¸ ì‘ìš©ì„ ì‹œê°ì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ê·¸ë˜í”„ í˜•ì‹ì˜ ë„êµ¬ì…ë‹ˆë‹¤. ì´ ë„êµ¬ëŠ” ë‹¤ì–‘í•œ ì–¸ì–´ ê°„ì˜ ê´€ê³„ë¥¼ ì‹œê°ì ìœ¼ë¡œ ì´í•´í•˜ê³  ë¶„ì„í•˜ëŠ” ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. LangGraphë¥¼ ì‚¬ìš©í•˜ë©´ ì–¸ì–´ ê°„ì˜ ê³µí†µì ê³¼ ì°¨ì´ì ì„ ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆìœ¼ë©°, ì–¸ì–´ ê°„ì˜ ìƒí˜¸ ì‘ìš©ì„ ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 147, 'prompt_tokens': 12, 'total_tokens': 159, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C9wCmL2xe5GTCjPjTrEIpJFUijODR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--b8c10ee7-f056-4e22-ba5d-a4057a8c4ec4-0', usage_metadata={'input_tokens': 12, 'output_tokens': 147, 'total_tokens': 159, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "# ì˜ˆ: ChatState(messages=...) íŒ¨í„´ + MemorySaver ì‚¬ìš© ì¤‘\n",
    "cfg = {\"configurable\": {\"thread_id\": \"session-1\"}}  # ì„¸ì…˜ ì‹ë³„ì\n",
    "\n",
    "# ê¸°ë³¸ ìŠ¤íŠ¸ë¦¼ (ê° ë…¸ë“œì˜ ì¶œë ¥ ë¬¶ìŒ ë‹¨ìœ„)\n",
    "for update in app.stream({\"messages\": [{\"type\":\"human\",\"content\":\"LangGraphë€?\"}]}, config=cfg):\n",
    "    print(\"ì¤‘ê°„ ì—…ë°ì´íŠ¸:\", update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a34523-a0b8-4f0c-8959-5da832a9a12c",
   "metadata": {},
   "source": [
    "### ğŸ“Œ Naive RAG\n",
    "- **ì „í†µì  êµ¬ì¡°**:\n",
    "  1. Retrieverë¡œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "  2. Generatorë¡œ LLM ì‘ë‹µ ìƒì„±\n",
    "- LangGraphëŠ” ì´ íë¦„ì„ **ì‹œê°ì ìœ¼ë¡œ, ëª¨ë“ˆì ìœ¼ë¡œ êµ¬ì„±** ê°€ëŠ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af75c46f-8899-430b-9ca1-2ace14cd743b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_5840\\1378141614.py:29: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(state[\"question\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== ë‹µë³€ ==\n",
      "LangGraphëŠ” ê·¸ë˜í”„ ê¸°ë°˜ LLM ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# pip install langgraph langchain-openai langchain-community faiss-cpu\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 1) ë°ì´í„° & ê²€ìƒ‰ê¸° ì¤€ë¹„ -------------------------------------------------------\n",
    "texts = [\n",
    "    \"LangGraphëŠ” ê·¸ë˜í”„ ê¸°ë°˜ LLM ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‹¤.\",\n",
    "    \"RAGëŠ” ê²€ìƒ‰(Retrieval)ê³¼ ìƒì„±(Generation)ì„ ê²°í•©í•œ ì ‘ê·¼ë²•ì´ë‹¤.\",\n",
    "    \"LangChainì€ LLM ì•± ê°œë°œì„ ì‰½ê²Œ í•˜ëŠ” í”„ë ˆì„ì›Œí¬ë‹¤.\",\n",
    "]\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vs = FAISS.from_texts(texts, emb)\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 3})  # â† NameError í•´ê²° í¬ì¸íŠ¸\n",
    "\n",
    "# 2) LLM ì¤€ë¹„ -------------------------------------------------------------------\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# 3) ìƒíƒœ ì •ì˜ ------------------------------------------------------------------\n",
    "class RAGState(TypedDict):\n",
    "    question: str\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "# 4) ë…¸ë“œ ì •ì˜ ------------------------------------------------------------------\n",
    "def retrieve_node(state: RAGState) -> RAGState:\n",
    "    docs = retriever.get_relevant_documents(state[\"question\"])\n",
    "    ctx = \"\\n\".join(d.page_content for d in docs)\n",
    "    state[\"context\"] = ctx\n",
    "    return state\n",
    "\n",
    "def generate_node(state: RAGState) -> RAGState:\n",
    "    prompt = f\"\"\"ë‹¹ì‹ ì€ í•œêµ­ì–´ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.\n",
    "ì§ˆë¬¸: {state['question']}\n",
    "ë¬¸ë§¥:\n",
    "{state['context']}\n",
    "\n",
    "ë¬¸ë§¥ì„ ê·¼ê±°ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\"\"\"\n",
    "    state[\"answer\"] = llm.invoke(prompt).content\n",
    "    return state\n",
    "\n",
    "# 5) ê·¸ë˜í”„ êµ¬ì„± ----------------------------------------------------------------\n",
    "graph = StateGraph(RAGState)\n",
    "graph.add_node(\"retrieve\", retrieve_node)\n",
    "graph.add_node(\"generate\", generate_node)\n",
    "graph.add_edge(START, \"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.add_edge(\"generate\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "# 6) ì‹¤í–‰ -----------------------------------------------------------------------\n",
    "out = app.invoke({\"question\": \"LangGraphë€?\"})\n",
    "print(\"== ë‹µë³€ ==\")\n",
    "print(out[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d90f51-9374-427f-8a59-f36cb1e49b86",
   "metadata": {},
   "source": [
    "### ğŸ“Œ í• ë£¨ì‹œë„¤ì´ì…˜ í‰ê°€ ëª¨ë“ˆ\n",
    "- ì‘ë‹µì— ê·¼ê±° ë¬¸ë§¥ì´ í¬í•¨ë˜ì§€ ì•Šìœ¼ë©´ ê²½ê³  ì¶”ê°€\n",
    "- **í’ˆì§ˆ ê´€ë¦¬ ë…¸ë“œ**ë¡œ í™œìš©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2315fca8-ae77-4b8c-abb4-845b0502cf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraphëŠ” ê·¸ë˜í”„ ê¸°ë°˜ LLM ì›Œí¬í”Œë¡œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# --- ê²€ìƒ‰/LLM ì¤€ë¹„ ---\n",
    "texts = [\n",
    "    \"LangGraphëŠ” ê·¸ë˜í”„ ê¸°ë°˜ LLM ì›Œí¬í”Œë¡œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‹¤.\",\n",
    "    \"RAGëŠ” ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ ì ‘ê·¼ë²•ì´ë‹¤.\",\n",
    "    \"LangChainì€ LLM ì•± ê°œë°œì„ ì‰½ê²Œ í•œë‹¤.\",\n",
    "]\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vs = FAISS.from_texts(texts, emb)\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 3})\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# --- ìƒíƒœ & ë…¸ë“œ ì •ì˜ ---\n",
    "class RAGState(TypedDict):\n",
    "    question: str\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "def retrieve_node(state: RAGState) -> RAGState:\n",
    "    docs = retriever.get_relevant_documents(state[\"question\"])\n",
    "    state[\"context\"] = \"\\n\".join(d.page_content for d in docs)\n",
    "    return state\n",
    "\n",
    "def generate_node(state: RAGState) -> RAGState:\n",
    "    prompt = f\"\"\"ì§ˆë¬¸: {state['question']}\n",
    "ë¬¸ë§¥:\n",
    "{state['context']}\n",
    "\n",
    "ë¬¸ë§¥ì„ ê·¼ê±°ë¡œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”.\"\"\"\n",
    "    state[\"answer\"] = llm.invoke(prompt).content\n",
    "    return state\n",
    "\n",
    "def hallucination_check(state: RAGState) -> RAGState:\n",
    "    # ì•„ì£¼ ë‹¨ìˆœí•œ ë°ëª¨ìš© ê·œì¹™ (ì‹¤ì „ì€ RAGAS/LLM ê²€ì¦ ë“± ê¶Œì¥)\n",
    "    if \"LangGraph\" not in state[\"answer\"]:\n",
    "        state[\"answer\"] += \" (ë¬¸ë§¥ê³¼ ë¬´ê´€í•œ ë‚´ìš©ì¼ ìˆ˜ ìˆìŒ)\"\n",
    "    return state\n",
    "\n",
    "# --- ê·¸ë˜í”„ êµ¬ì„±(compile ì „ì— ì „ë¶€ ì¶”ê°€) ---\n",
    "graph = StateGraph(RAGState)\n",
    "graph.add_node(\"retrieve\", retrieve_node)\n",
    "graph.add_node(\"generate\", generate_node)\n",
    "graph.add_node(\"check\", hallucination_check)\n",
    "\n",
    "graph.add_edge(START, \"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.add_edge(\"generate\", \"check\")\n",
    "graph.add_edge(\"check\", END)  # â† ì¢…ë£ŒëŠ” ENDë¡œ ë‘ëŠ” í¸ì´ ìµœì‹  íŒ¨í„´ì— ë§ìŒ\n",
    "\n",
    "# ë°˜ë“œì‹œ ì—¬ê¸°ì„œ ì»´íŒŒì¼\n",
    "app = graph.compile()\n",
    "\n",
    "# ì‹¤í–‰\n",
    "out = app.invoke({\"question\": \"LangGraphë€?\"})\n",
    "print(out[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a0d65-b83f-4dce-9b85-305eba219c0c",
   "metadata": {},
   "source": [
    "### ğŸ“Œ ì›¹ ê²€ìƒ‰ ë…¸ë“œ\n",
    "- Retriever ì™¸ë¶€ì— **ì‹¤ì‹œê°„ ê²€ìƒ‰ ë…¸ë“œ** ì¶”ê°€ ê°€ëŠ¥\n",
    "- ìµœì‹  ì •ë³´ ê¸°ë°˜ RAG êµ¬í˜„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e09ab148-4c67-4c4a-8206-de7e7fa140cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraphëŠ” ì–¸ì–´ ê°„ ìƒí˜¸ ì—°ê²°ì„±ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ê·¸ë˜í”„ ì‹œê°í™” ë„êµ¬ì…ë‹ˆë‹¤. LangGraphëŠ” ê·¸ë˜í”„ ê¸°ë°˜ LLM ì›Œí¬í”Œë¡œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë©°, LangChainì€ LLM ì•± ê°œë°œì„ ì‰½ê²Œ ë„ì™€ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. RAGëŠ” ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”ì‹œ ì„¤ì¹˜: pip install duckduckgo-search langgraph langchain-openai langchain-community faiss-cpu\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# ----- 0) ë¦¬ì†ŒìŠ¤ ì¤€ë¹„ -----\n",
    "texts = [\n",
    "    \"LangGraphëŠ” ê·¸ë˜í”„ ê¸°ë°˜ LLM ì›Œí¬í”Œë¡œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‹¤.\",\n",
    "    \"RAGëŠ” ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ ì ‘ê·¼ë²•ì´ë‹¤.\",\n",
    "    \"LangChainì€ LLM ì•± ê°œë°œì„ ì‰½ê²Œ í•œë‹¤.\",\n",
    "]\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vs = FAISS.from_texts(texts, emb)\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "ddg = DuckDuckGoSearchRun()\n",
    "\n",
    "# ----- 1) ìƒíƒœ ì •ì˜ -----\n",
    "class RAGState(TypedDict):\n",
    "    question: str\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "# ----- 2) ë…¸ë“œ ì •ì˜ -----\n",
    "def rewrite_node(state: RAGState) -> RAGState:\n",
    "    # (ì˜µì…˜) ì¿¼ë¦¬ ì¬ì‘ì„±: í•„ìš” ì—†ìœ¼ë©´ ì´ ë…¸ë“œ/ì—£ì§€ ì œê±°í•´ë„ ë©ë‹ˆë‹¤.\n",
    "    new_q = llm.invoke(f\"ë‹¤ìŒ ì§ˆë¬¸ì„ ì›¹/ë¬¸ì„œ ê²€ìƒ‰ì— ì í•©í•˜ê²Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¬ì‘ì„±: {state['question']}\").content\n",
    "    state[\"question\"] = new_q.strip()\n",
    "    return state\n",
    "\n",
    "def websearch_node(state: RAGState) -> RAGState:\n",
    "    # DuckDuckGo ì›¹ê²€ìƒ‰ ê²°ê³¼ ë¬¸ìì—´(ìš”ì•½)ì„ contextì— ì¶”ê°€\n",
    "    result = ddg.run(state[\"question\"])\n",
    "    state[\"context\"] = (state.get(\"context\") or \"\") + (\"\\n\" if state.get(\"context\") else \"\") + result\n",
    "    return state\n",
    "\n",
    "def retrieve_node(state: RAGState) -> RAGState:\n",
    "    docs = retriever.get_relevant_documents(state[\"question\"])\n",
    "    ctx = \"\\n\".join(d.page_content for d in docs)\n",
    "    state[\"context\"] = (state.get(\"context\") + \"\\n\" if state.get(\"context\") else \"\") + ctx\n",
    "    return state\n",
    "\n",
    "def generate_node(state: RAGState) -> RAGState:\n",
    "    prompt = f\"\"\"ë‹¹ì‹ ì€ í•œêµ­ì–´ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.\n",
    "ì§ˆë¬¸: {state['question']}\n",
    "ë¬¸ë§¥:\n",
    "{state['context']}\n",
    "\n",
    "ë¬¸ë§¥ì„ ê·¼ê±°ë¡œ ê°„ê²°í•˜ê³  ì‚¬ì‹¤ì ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\"\"\"\n",
    "    state[\"answer\"] = llm.invoke(prompt).content\n",
    "    return state\n",
    "\n",
    "def hallucination_check(state: RAGState) -> RAGState:\n",
    "    # ë°ëª¨ìš© ê°„ë‹¨ ê²€ì¦\n",
    "    if \"LangGraph\" not in state[\"answer\"] and \"LangGraph\" in state[\"context\"]:\n",
    "        state[\"answer\"] += \" (ë¬¸ë§¥ê³¼ ë¬´ê´€í•  ìˆ˜ ìˆìŒ)\"\n",
    "    return state\n",
    "\n",
    "# ----- 3) ê·¸ë˜í”„ êµ¬ì„± -----\n",
    "graph = StateGraph(RAGState)\n",
    "graph.add_node(\"rewrite\", rewrite_node)          # (ì˜µì…˜)\n",
    "graph.add_node(\"websearch\", websearch_node)      # ìƒˆ ë…¸ë“œ\n",
    "graph.add_node(\"retrieve\", retrieve_node)\n",
    "graph.add_node(\"generate\", generate_node)\n",
    "graph.add_node(\"check\", hallucination_check)\n",
    "\n",
    "# íë¦„: START â†’ (rewrite) â†’ websearch â†’ retrieve â†’ generate â†’ check â†’ END\n",
    "graph.add_edge(START, \"rewrite\")\n",
    "graph.add_edge(\"rewrite\", \"websearch\")\n",
    "graph.add_edge(\"websearch\", \"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.add_edge(\"generate\", \"check\")\n",
    "graph.add_edge(\"check\", END)\n",
    "\n",
    "# ----- 4) ë°˜ë“œì‹œ ì—¬ê¸°ì„œ ì»´íŒŒì¼! -----\n",
    "app = graph.compile()\n",
    "\n",
    "# ----- 5) ì‹¤í–‰ -----\n",
    "out = app.invoke({\"question\": \"LangGraphë€?\"})\n",
    "print(out[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac765cc-6fcb-4c62-9052-caee8a2b6399",
   "metadata": {},
   "source": [
    "### ğŸ“Œ ì¿¼ë¦¬ ì¬ì‘ì„± ë…¸ë“œ\n",
    "- ì›ë³¸ ì§ˆë¬¸ì„ ê²€ìƒ‰ ì¹œí™”ì ìœ¼ë¡œ ì¬ì‘ì„±\n",
    "- ê²€ìƒ‰ Recall í–¥ìƒ â†’ RAG ì„±ëŠ¥ ê°œì„ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "777fad22-e943-4bd6-8e57-62162bfb5d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraphëŠ” Multi-Agent íŠ¹í™” í”„ë ˆì„ì›Œí¬ë¡œ, ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. ê° ì—ì´ì „íŠ¸ëŠ” íŠ¹ì • ëª©í‘œë¥¼ í–¥í•´ ììœ¨ì ìœ¼ë¡œ í–‰ë™í•˜ë©°, ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë“¤ê³¼ í˜‘ì—…í•˜ì—¬ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. LangGraphëŠ” AIì˜ ì‚¬ê³  íë¦„ì„ ê·¸ë¦´ ìˆ˜ ìˆëŠ” ë„êµ¬ë¡œ, ë³µì¡í•œ AI ì‹œìŠ¤í…œì„ ì‰½ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# pip install langgraph langchain-openai langchain-community faiss-cpu duckduckgo-search\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# ë¦¬ì†ŒìŠ¤\n",
    "texts = [\n",
    "    \"LangGraphëŠ” ê·¸ë˜í”„ ê¸°ë°˜ LLM ì›Œí¬í”Œë¡œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‹¤.\",\n",
    "    \"RAGëŠ” ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ ì ‘ê·¼ë²•ì´ë‹¤.\",\n",
    "    \"LangChainì€ LLM ì•± ê°œë°œì„ ì‰½ê²Œ í•œë‹¤.\",\n",
    "]\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vs = FAISS.from_texts(texts, emb)\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "ddg = DuckDuckGoSearchRun()\n",
    "\n",
    "# ìƒíƒœ\n",
    "class RAGState(TypedDict):\n",
    "    question: str\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "# ë…¸ë“œ\n",
    "def rewrite_query(state: RAGState) -> RAGState:\n",
    "    state[\"question\"] = llm.invoke(\n",
    "        f\"ë‹¤ìŒ ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìµœì í™”ëœ ì¿¼ë¦¬ë¡œ ë‹¤ì‹œ ì¨ì¤˜: {state['question']}\"\n",
    "    ).content.strip()\n",
    "    return state\n",
    "\n",
    "def websearch_node(state: RAGState) -> RAGState:\n",
    "    result = ddg.run(state[\"question\"])\n",
    "    state[\"context\"] = (state.get(\"context\") + \"\\n\" if state.get(\"context\") else \"\") + result\n",
    "    return state\n",
    "\n",
    "def retrieve_node(state: RAGState) -> RAGState:\n",
    "    docs = retriever.get_relevant_documents(state[\"question\"])\n",
    "    ctx = \"\\n\".join(d.page_content for d in docs)\n",
    "    state[\"context\"] = (state.get(\"context\") + \"\\n\" if state.get(\"context\") else \"\") + ctx\n",
    "    return state\n",
    "\n",
    "def generate_node(state: RAGState) -> RAGState:\n",
    "    prompt = f\"\"\"ë‹¹ì‹ ì€ í•œêµ­ì–´ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.\n",
    "ì§ˆë¬¸: {state['question']}\n",
    "ë¬¸ë§¥:\n",
    "{state['context']}\n",
    "\n",
    "ë¬¸ë§¥ì„ ê·¼ê±°ë¡œ ê°„ê²°í•˜ê³  ì‚¬ì‹¤ì ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\"\"\"\n",
    "    state[\"answer\"] = llm.invoke(prompt).content\n",
    "    return state\n",
    "\n",
    "def hallucination_check(state: RAGState) -> RAGState:\n",
    "    if \"LangGraph\" not in state[\"answer\"] and \"LangGraph\" in state.get(\"context\", \"\"):\n",
    "        state[\"answer\"] += \" (ë¬¸ë§¥ê³¼ ë¬´ê´€í•  ìˆ˜ ìˆìŒ)\"\n",
    "    return state\n",
    "\n",
    "# ê·¸ë˜í”„ êµ¬ì„± (ëª¨ë‘ ì¶”ê°€ â†’ ë§ˆì§€ë§‰ì— ì»´íŒŒì¼)\n",
    "graph = StateGraph(RAGState)\n",
    "graph.add_node(\"rewrite\", rewrite_query)\n",
    "graph.add_node(\"websearch\", websearch_node)\n",
    "graph.add_node(\"retrieve\", retrieve_node)\n",
    "graph.add_node(\"generate\", generate_node)\n",
    "graph.add_node(\"check\", hallucination_check)\n",
    "\n",
    "graph.add_edge(START, \"rewrite\")\n",
    "graph.add_edge(\"rewrite\", \"websearch\")\n",
    "graph.add_edge(\"websearch\", \"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"generate\")\n",
    "graph.add_edge(\"generate\", \"check\")\n",
    "graph.add_edge(\"check\", END)\n",
    "\n",
    "app = graph.compile()  # â† í•œ ë²ˆë§Œ ì»´íŒŒì¼\n",
    "\n",
    "# ì‹¤í–‰\n",
    "print(app.invoke({\"question\": \"LangGraphë€?\"})[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3209b934-e393-4cfe-b8ab-f5e93d7aeedb",
   "metadata": {},
   "source": [
    "## âœ… ìµœì¢… ì •ë¦¬\n",
    "\n",
    "- LangGraph ê°œìš”: State, Node, Edge, Compile â†’ ì²´ì¸ë³´ë‹¤ ê°•ë ¥í•œ ì›Œí¬í”Œë¡œìš° ì—”ì§„\n",
    "\n",
    "- í•µì‹¬ ê¸°ëŠ¥: TypedDict, Annotated, Reducer / Memory(Checkpointer) / Stream / Interrupt / Replay\n",
    "\n",
    "- êµ¬ì¡° ì„¤ê³„: Naive RAG â†’ í‰ê°€ ëª¨ë“ˆ â†’ ì›¹ê²€ìƒ‰ â†’ ì¿¼ë¦¬ ì¬ì‘ì„± ë“± ëª¨ë“ˆ ë‹¨ìœ„ í™•ì¥ ê°€ëŠ¥\n",
    "\n",
    "- ğŸ‘‰ LangGraphëŠ” ë‹¨ìˆœ ì²´ì¸ì—ì„œ í•œ ë‹¨ê³„ ë” ë‚˜ì•„ê°€, ë³µì¡í•œ RAG/ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ ì—”ì§€ë‹ˆì–´ë§í•˜ëŠ” í•„ìˆ˜ ë„êµ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d9f57-94e7-40eb-9b09-a0c6e1842db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_new_env)",
   "language": "python",
   "name": "my_new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
