{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bdc193d-1fdc-449d-b91d-3cf9a88898dc",
   "metadata": {},
   "source": [
    "### 📌 01. LLM 과 RAG 평가 방식, 평가 지표\n",
    "- **RAG 평가 목표**: 검색된 문서 + LLM 응답 품질 측정\n",
    "- 주요 평가 지표:\n",
    "  - **Precision**: 검색된 문서 중 얼마나 관련 있는가\n",
    "  - **Recall**: 실제 관련 문서 중 얼마나 검색되었는가\n",
    "  - **Faithfulness**: 답변이 근거 문서에 충실한가\n",
    "  - **Relevancy**: 답변이 질문과 얼마나 관련 있는가\n",
    "- 👉 단순 정확도(Accuracy) 대신, 맥락 기반 평가 필요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81599d09-c03e-47ff-9321-18f29706da4b",
   "metadata": {},
   "source": [
    "### 📌 02. RAGAS 소개\n",
    "- **RAGAS (RAG Assessment Suite)**: RAG 평가 전용 툴킷\n",
    "- HuggingFace `datasets` 기반으로 작동\n",
    "- 평가 지표:\n",
    "  - `context_precision`\n",
    "  - `context_recall`\n",
    "  - `answer_relevancy`\n",
    "  - `faithfulness`\n",
    "- LLM 기반 자동 평가 지원 → 휴먼 평가 대체/보완\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a92248-898e-444e-b958-ab3df813698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  RAGAS 소개\n",
    "# # pip install ragas\n",
    "\n",
    "# from datasets import Dataset\n",
    "# from ragas.metrics import context_precision, context_recall, answer_relevancy, faithfulness\n",
    "\n",
    "# # 간단한 테스트 데이터\n",
    "# data = {\n",
    "#     \"question\": [\"고양이는 어디에서 많이 키우나?\"],\n",
    "#     \"answer\": [\"고양이는 집에서 많이 키운다.\"],\n",
    "#     \"contexts\": [[\"고양이는 귀엽다.\", \"강아지는 충성스럽다.\"]]\n",
    "# }\n",
    "\n",
    "# dataset = Dataset.from_dict(data)\n",
    "\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a630a52-9c7b-4f83-a566-14be5ac701c0",
   "metadata": {},
   "source": [
    "### 📌 03. 합성 테스트 데이터셋 생성\n",
    "- 실제 데이터 부족 → **합성 테스트셋(Synthetic Test Set)** 생성\n",
    "- 방법:\n",
    "  - 원문 문서 → LLM이 질문 생성\n",
    "  - 질문에 대한 정답 생성\n",
    "- 장점:\n",
    "  - 자동화, 빠른 실험 가능\n",
    "- 단점:\n",
    "  - LLM이 만든 데이터 → 품질 검증 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b93ff6a-2263-4344-a942-199841d7ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 합성 테스트 데이터셋 생성 (Synthetic Dataset)\n",
    "# # 예시: 질문-답변 쌍을 LLM으로 자동 생성\n",
    "\n",
    "# from ragas.testset import TestsetGenerator\n",
    "\n",
    "# generator = TestsetGenerator(llm=llm, embeddings=embeddings)\n",
    "# docs = [\"고양이는 귀엽다.\", \"강아지는 충성스럽다.\", \"호랑이는 멋지다.\"]\n",
    "\n",
    "# # 문서 기반 synthetic dataset 생성\n",
    "# testset = generator.generate_with_langchain_docs(docs, test_size=3)\n",
    "\n",
    "# print(testset.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97492c9-123e-471b-8c6f-0bf8bc7f0fee",
   "metadata": {},
   "source": [
    "### 📌 04. RAGAS 평가 (Context Precision, Recall, Relevancy, Faithfulness)\n",
    "- **Context Precision**: 검색된 문서 중 관련된 비율\n",
    "- **Context Recall**: 관련 문서 중 검색된 비율\n",
    "- **Answer Relevancy**: 답변이 질문에 적합한가\n",
    "- **Faithfulness**: 답변이 문서에 충실한가\n",
    "- 👉 네 지표를 종합적으로 활용해야 RAG 성능 평가 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b46471-e7cf-4c9d-b37c-08e78b1c9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RAGAS 를 활용한 평가\n",
    "# from ragas import evaluate\n",
    "\n",
    "# result = evaluate(\n",
    "#     dataset,\n",
    "#     metrics=[context_precision, context_recall, answer_relevancy, faithfulness],\n",
    "#     llm=llm,\n",
    "#     embeddings=embeddings\n",
    "# )\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d342f4c6-1966-40e9-88c9-aa2071d12ba2",
   "metadata": {},
   "source": [
    "### 📌 05. 테스트 데이터셋 번역/업로드 관리\n",
    "- RAGAS는 영어 데이터셋에 최적화 → 한국어도 번역 후 평가 가능\n",
    "- HuggingFace Dataset 포맷:\n",
    "  - question\n",
    "  - answer\n",
    "  - contexts\n",
    "- 관리 포인트:\n",
    "  - 여러 언어 → 통일된 언어로 변환\n",
    "  - HuggingFace Hub 업로드/공유 가능\n",
    "- 👉 실제 운영 환경에서는 **다국어 평가 파이프라인** 구축 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ccc66e5-c9f5-440b-862a-93e91048d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 테스트 데이터셋 번역/업로드 관리\n",
    "# # 예: 한국어 → 영어 번역 후 평가 가능\n",
    "\n",
    "# from datasets import Dataset\n",
    "\n",
    "# data = {\n",
    "#     \"question\": [\"한국의 수도는 어디인가?\"],\n",
    "#     \"answer\": [\"서울은 한국의 수도이다.\"],\n",
    "#     \"contexts\": [[\"서울은 대한민국의 수도이다.\", \"도쿄는 일본의 수도이다.\"]]\n",
    "# }\n",
    "# dataset = Dataset.from_dict(data)\n",
    "\n",
    "# # 번역 (예시: LLM 이용)\n",
    "# translation_prompt = \"Translate this question to English: 한국의 수도는 어디인가?\"\n",
    "# print(llm.invoke(translation_prompt).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19b6e01-fc81-4159-8c19-c7bfa6bb7b43",
   "metadata": {},
   "source": [
    "# ✅ 최종 정리\n",
    "- **RAG 평가 지표**: Precision, Recall, Relevancy, Faithfulness\n",
    "- **RAGAS**: 자동화된 RAG 평가 프레임워크\n",
    "- **Synthetic Testset**: 문서 기반 자동 질문/답변 생성\n",
    "- **평가 실행**: `ragas.evaluate` 사용\n",
    "- **번역/업로드 관리**: 다국어 지원 위해 사전 번역 및 Dataset Hub 활용\n",
    "\n",
    "👉 RAG 평가는 단순 정확도 → 맥락 기반 종합 지표로 넘어가는 중.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0edb73f-8610-4633-a0f0-55fe9453ac6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_new_env)",
   "language": "python",
   "name": "my_new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
