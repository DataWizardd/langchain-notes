{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a7752d-1c0d-47e3-95dc-b5bc2fe20c21",
   "metadata": {},
   "source": [
    "# ğŸ“˜ ë©”ëª¨ë¦¬ (Memory)\n",
    "\n",
    "---\n",
    "\n",
    "## 01. ë©”ëª¨ë¦¬ë€ (00:02:02)\n",
    "\n",
    "### ğŸ“Œ ê°œë…\n",
    "- ê¸°ë³¸ì ìœ¼ë¡œ LLMì€ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ì§€ ëª»í•¨  \n",
    "- **Memory**ë¥¼ í™œìš©í•˜ë©´ ëŒ€í™” ê¸°ë¡ì„ ë³´ê´€/ì¬ì‚¬ìš© ê°€ëŠ¥  \n",
    "- ì£¼ìš” ë©”ëª¨ë¦¬ ìœ í˜•:\n",
    "  - Buffer (ì „ì²´ ëŒ€í™” ì €ì¥)\n",
    "  - Window (ìµœê·¼ nê°œ ì €ì¥)\n",
    "  - TokenBuffer (í† í° ë‹¨ìœ„ ì €ì¥)\n",
    "  - Entity (ê°œì²´ ì¤‘ì‹¬ ì €ì¥)\n",
    "  - Knowledge Graph (ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜)\n",
    "  - Summary (ìš”ì•½ ê¸°ë°˜ ì €ì¥)\n",
    "  - VectorStore (ë²¡í„° DB ê¸°ë°˜ ì €ì¥)\n",
    "\n",
    "---\n",
    "\n",
    "## 02. ConversationBufferMemory (00:10:33)\n",
    "\n",
    "### ğŸ“Œ ê°œë…\n",
    "- **ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬**ë¥¼ ì €ì¥  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc25f6e-a497-40a1-a339-972c509b7c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-73\n"
     ]
    }
   ],
   "source": [
    "### ğŸ’» ì˜ˆì‹œ ì½”ë“œ\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(api_key[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5341b58-0911-498d-93ea-f7b87009cced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_8688\\2177794017.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "C:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_8688\\2177794017.py:7: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  chain = ConversationChain(llm=llm, memory=memory, verbose=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ì•ˆë…•?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'ì•ˆë…•?', 'history': '', 'response': 'ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì±—ë´‡ì…ë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: ì•ˆë…•?\n",
      "AI: ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì±—ë´‡ì…ë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
      "Human: ë‚´ê°€ ë°©ê¸ˆ ë­ë¼ê³  í–ˆì§€?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'ë‚´ê°€ ë°©ê¸ˆ ë­ë¼ê³  í–ˆì§€?', 'history': 'Human: ì•ˆë…•?\\nAI: ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì±—ë´‡ì…ë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?', 'response': 'ì•ˆë…•?'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "memory = ConversationBufferMemory()\n",
    "chain = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "print(chain.invoke(\"ì•ˆë…•?\"))\n",
    "print(chain.invoke(\"ë‚´ê°€ ë°©ê¸ˆ ë­ë¼ê³  í–ˆì§€?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12c355-532c-4ed3-909d-83e4922e59e8",
   "metadata": {},
   "source": [
    "## 03. ConversationBufferWindowMemory \n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- ìµœê·¼ nê°œ ëŒ€í™”ë§Œ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c74de3a-735e-4579-9c70-3de4cf77fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_8688\\19630597.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=1)  # ìµœê·¼ 1ê°œë§Œ ê¸°ì–µ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ë‚˜ëŠ” ì² ìˆ˜ì•¼.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'ë‚˜ëŠ” ì² ìˆ˜ì•¼.', 'history': '', 'response': 'ì•ˆë…• ì² ìˆ˜ì•¼! ë‚˜ëŠ” ì¸ê³µì§€ëŠ¥ì´ì•¼. ë„ˆì˜ ì´ë¦„ì€ ë©‹ì§€ë‹¤. ì–´ë–»ê²Œ ì§€ë‚´?'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: ë‚˜ëŠ” ì² ìˆ˜ì•¼.\n",
      "AI: ì•ˆë…• ì² ìˆ˜ì•¼! ë‚˜ëŠ” ì¸ê³µì§€ëŠ¥ì´ì•¼. ë„ˆì˜ ì´ë¦„ì€ ë©‹ì§€ë‹¤. ì–´ë–»ê²Œ ì§€ë‚´?\n",
      "Human: ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?', 'history': 'Human: ë‚˜ëŠ” ì² ìˆ˜ì•¼.\\nAI: ì•ˆë…• ì² ìˆ˜ì•¼! ë‚˜ëŠ” ì¸ê³µì§€ëŠ¥ì´ì•¼. ë„ˆì˜ ì´ë¦„ì€ ë©‹ì§€ë‹¤. ì–´ë–»ê²Œ ì§€ë‚´?', 'response': 'ë„¤ ì´ë¦„ì€ ì² ìˆ˜ì•¼. ì €ë²ˆì— ë§í–ˆì—ˆì–´ìš”. ì§€ê¸ˆì€ ì–´ë–»ê²Œ ì§€ë‚´?'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)  # ìµœê·¼ 1ê°œë§Œ ê¸°ì–µ\n",
    "chain = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "print(chain.invoke(\"ë‚˜ëŠ” ì² ìˆ˜ì•¼.\"))\n",
    "print(chain.invoke(\"ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56846db-49d7-4b66-89f5-24359126286b",
   "metadata": {},
   "source": [
    "## 04. ConversationTokenBufferMemory \n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- í† í° ë‹¨ìœ„ë¡œ íˆìŠ¤í† ë¦¬ ì €ì¥\n",
    "\n",
    "- ì§€ì •í•œ í† í° ê¸¸ì´ë¥¼ ë„˜ìœ¼ë©´ ì˜¤ë˜ëœ ëŒ€í™” ì‚­ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a8f1a8d-fccf-43fc-905a-aab981e06ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_8688\\1779030638.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ì˜¤ëŠ˜ ì ì‹¬ì€ íŒŒìŠ¤íƒ€ ë¨¹ì—ˆì–´.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'ì˜¤ëŠ˜ ì ì‹¬ì€ íŒŒìŠ¤íƒ€ ë¨¹ì—ˆì–´.', 'history': '', 'response': 'íŒŒìŠ¤íƒ€ëŠ” ì •ë§ ë§›ìˆëŠ” ì„ íƒì´ì—ìš”! ì–´ë–¤ ì¢…ë¥˜ì˜ íŒŒìŠ¤íƒ€ë¥¼ ë¨¹ì—ˆë‚˜ìš”? í¬ë¦¼ì†ŒìŠ¤, í† ë§ˆí† ì†ŒìŠ¤, í˜¹ì€ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ì†ŒìŠ¤ë¥¼ ì„ íƒí–ˆë‚˜ìš”?'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ë‚´ê°€ ë­ ë¨¹ì—ˆë‹¤ê³  í–ˆì§€?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'ë‚´ê°€ ë­ ë¨¹ì—ˆë‹¤ê³  í–ˆì§€?', 'history': '', 'response': 'ì£„ì†¡í•©ë‹ˆë‹¤, ê·¸ ì •ë³´ëŠ” ì œê°€ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë¨¹ì—ˆëŠ”ì§€ ìì„¸íˆ ë§ì”€í•´ì£¼ì‹œë©´ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "chain = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "print(chain.invoke(\"ì˜¤ëŠ˜ ì ì‹¬ì€ íŒŒìŠ¤íƒ€ ë¨¹ì—ˆì–´.\"))\n",
    "print(chain.invoke(\"ë‚´ê°€ ë­ ë¨¹ì—ˆë‹¤ê³  í–ˆì§€?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1b8e65-eaf6-4e61-82d4-4df4051afc99",
   "metadata": {},
   "source": [
    "## 05. ConversationEntityMemory\n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- ëŒ€í™”ì—ì„œ ê°œì²´(entity)ë¥¼ ì¶”ì¶œí•´ì„œ ê¸°ì–µ\n",
    "\n",
    "- ì‚¬ëŒ, ì¥ì†Œ, ì¡°ì§ ê°™ì€ ì •ë³´ ìœ ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa1cf99-7465-40bd-a79f-fb4141ff8c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.memory import ConversationEntityMemory\n",
    "\n",
    "# memory = ConversationEntityMemory(llm=llm)\n",
    "# chain = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "# print(chain.invoke(\"ë‚´ ì¹œêµ¬ëŠ” ì˜í¬ì•¼.\"))\n",
    "# print(chain.invoke(\"ë‚´ ì¹œêµ¬ ì´ë¦„ì´ ë­ì§€?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795abbfd-c355-4172-8aad-36b5796ae95c",
   "metadata": {},
   "source": [
    "## 06. ConversationKGMemory\n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- ëŒ€í™”ì—ì„œ ì§€ì‹ ê·¸ë˜í”„(Knowledge Graph) í˜•íƒœë¡œ ê´€ê³„ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9790254b-ea8d-4629-8227-8612ab824862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.memory import ConversationKGMemory\n",
    "\n",
    "# memory = ConversationKGMemory(llm=llm)\n",
    "# chain = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "# print(chain.invoke(\"ì² ìˆ˜ëŠ” ì˜í¬ì˜ ì¹œêµ¬ì•¼.\"))\n",
    "# print(chain.invoke(\"ì² ìˆ˜ì™€ ì˜í¬ëŠ” ì–´ë–¤ ê´€ê³„ì§€?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb5a8d-fd94-4538-a260-03aec14cbd68",
   "metadata": {},
   "source": [
    "## 07. ConversationSummaryMemory \n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- ëŒ€í™”ê°€ ê¸¸ì–´ì§€ë©´ ì „ì²´ ì €ì¥ ëŒ€ì‹  ìš”ì•½í•´ì„œ ìœ ì§€\n",
    "\n",
    "- ê¸´ ëŒ€í™”ì—ë„ ìœ ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af26ec10-3f41-44ec-9163-eff34aab171e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_8688\\3963364838.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ì˜¤ëŠ˜ ë‚˜ëŠ” ì˜í™”ê´€ì— ê°”ì–´. ì•¡ì…˜ ì˜í™”ì˜€ì–´.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'ì˜¤ëŠ˜ ë‚˜ëŠ” ì˜í™”ê´€ì— ê°”ì–´. ì•¡ì…˜ ì˜í™”ì˜€ì–´.', 'history': '', 'response': 'ì •ë§ìš”? ì–´ë–¤ ì˜í™”ë¥¼ ë³´ì…¨ë‚˜ìš”? ì €ëŠ” ì˜í™”ì— ëŒ€í•´ ë§ì€ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì–´ìš”.'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human tells the AI they went to the movie theater today to watch an action movie. The AI asks for more details about the movie, mentioning it has a lot of information about films.\n",
      "Human: ë‚´ê°€ ì˜¤ëŠ˜ ë­ í–ˆì§€?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'ë‚´ê°€ ì˜¤ëŠ˜ ë­ í–ˆì§€?', 'history': 'The human tells the AI they went to the movie theater today to watch an action movie. The AI asks for more details about the movie, mentioning it has a lot of information about films.', 'response': 'ì˜í™”ê´€ì— ê°€ì„œ ì•¡ì…˜ ì˜í™”ë¥¼ ë³´ëŸ¬ ê°”ë‹¤ê³  í–ˆë„¤ìš”! ì–´ë–¤ ì˜í™”ë¥¼ ë³´ì…¨ëŠ”ì§€ ìì„¸íˆ ë§í•´ì£¼ì‹¤ë˜ìš”? ì˜í™”ì— ëŒ€í•´ ë§ì€ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì–´ìš”.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "chain = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
    "\n",
    "print(chain.invoke(\"ì˜¤ëŠ˜ ë‚˜ëŠ” ì˜í™”ê´€ì— ê°”ì–´. ì•¡ì…˜ ì˜í™”ì˜€ì–´.\"))\n",
    "print(chain.invoke(\"ë‚´ê°€ ì˜¤ëŠ˜ ë­ í–ˆì§€?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7057d76-3559-4344-a189-ae0f4809384a",
   "metadata": {},
   "source": [
    "## 08. VectorStoreRetrieverMemory\n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- ëŒ€í™” ë‚´ìš©ì„ ë²¡í„° DBì— ì €ì¥ í›„ ê²€ìƒ‰\n",
    "\n",
    "- ê¸´ ëŒ€í™”ë‚˜ ì™¸ë¶€ ì§€ì‹ ì—°ë™ ì‹œ ìœ ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77818e4f-b6e2-4d9f-a084-ba6788db8065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.memory import VectorStoreRetrieverMemory\n",
    "# from langchain.vectorstores import FAISS\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# vectorstore = FAISS.from_texts([\"ìƒ˜í”Œ ë°ì´í„°\"], embeddings)\n",
    "\n",
    "# retriever = vectorstore.as_retriever()\n",
    "# memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "# memory.save_context({\"input\": \"ë‚´ ì´ë¦„ì€ ì² ìˆ˜ì•¼\"}, {\"output\": \"ì•Œê² ì–´\"})\n",
    "# print(memory.load_memory_variables({\"input\": \"ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5436fe48-457e-4e27-8386-37c2be842c75",
   "metadata": {},
   "source": [
    "## 09. LCEL Chain ì— ë©”ëª¨ë¦¬ ì¶”ê°€ \n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- LCEL(LangChain Expression Language)ë¡œ ì²´ì¸ êµ¬ì„± ì‹œ ë©”ëª¨ë¦¬ ì¶”ê°€ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d85c9f64-ec96-4ea7-a2c1-84969ad47df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"ë„ˆëŠ” ê¸°ì–µë ¥ì´ ì¢‹ì€ ì–´ì‹œìŠ¤í„´íŠ¸ì•¼.\"),\n",
    "#     (\"human\", \"{input}\")\n",
    "# ])\n",
    "\n",
    "# memory = ConversationBufferMemory(return_messages=True)\n",
    "# chain = prompt | llm | memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f90cfb-c555-4030-a581-b3930f6f77b6",
   "metadata": {},
   "source": [
    "## 10. SQLite ì— ëŒ€í™”ë‚´ìš© (ì‚¬ìš©ì ë° ëŒ€í™”ë³„) ì €ì¥ \n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- ë©”ëª¨ë¦¬ë¥¼ SQLite DBì— ì €ì¥í•´ ì„¸ì…˜ë³„ ê¸°ë¡ ê´€ë¦¬ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "964a02c1-0938-47f6-ada1-92ad9d2909cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.memory import SQLiteChatMessageHistory\n",
    "\n",
    "# history = SQLiteChatMessageHistory(database_path=\"chat_memory.db\", session_id=\"user1\")\n",
    "# history.add_user_message(\"ì•ˆë…•?\")\n",
    "# history.add_ai_message(\"ì•ˆë…•í•˜ì„¸ìš”!\")\n",
    "\n",
    "# print(history.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34fd752-395a-40b3-a0c4-cba71410123e",
   "metadata": {},
   "source": [
    "## 11. ì¼ë°˜ ë³€ìˆ˜ì— ëŒ€í™”ë‚´ìš© ì €ì¥(íœ˜ë°œì„± ë©”ëª¨ë¦¬)\n",
    "### ğŸ“Œ ê°œë…\n",
    "\n",
    "- ëŒ€í™” ë‚´ìš©ì„ íŒŒì´ì¬ ë³€ìˆ˜(list ë“±)ì— ì €ì¥í•˜ëŠ” ë‹¨ìˆœ ë©”ëª¨ë¦¬\n",
    "\n",
    "- í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ íœ˜ë°œì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "410302ba-8a86-4a11-877c-162739677a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'user': 'ì•ˆë…•?', 'ai': 'ì•ˆë…•í•˜ì„¸ìš”!'}, {'user': 'ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?', 'ai': 'ì˜¤ëŠ˜ì€ ë§‘ìŠµë‹ˆë‹¤.'}]\n"
     ]
    }
   ],
   "source": [
    "conversation_log = []\n",
    "\n",
    "def chat(user_input, ai_response):\n",
    "    conversation_log.append({\"user\": user_input, \"ai\": ai_response})\n",
    "\n",
    "chat(\"ì•ˆë…•?\", \"ì•ˆë…•í•˜ì„¸ìš”!\")\n",
    "chat(\"ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?\", \"ì˜¤ëŠ˜ì€ ë§‘ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(conversation_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af5852-090d-403b-99c4-75c29d816ad5",
   "metadata": {},
   "source": [
    "## âœ… ì •ë¦¬\n",
    "\n",
    "- Buffer / Window / TokenBuffer â†’ ëŒ€í™” ê¸¸ì´ ì œì–´ ë°©ì‹\n",
    "\n",
    "- Entity / KG / Summary â†’ ì˜ë¯¸ ì¤‘ì‹¬ ì €ì¥\n",
    "\n",
    "- VectorStore â†’ ë²¡í„° DB ê¸°ë°˜ ê²€ìƒ‰\n",
    "\n",
    "- SQLite â†’ ì˜êµ¬ ì €ì¥\n",
    "\n",
    "- ì¼ë°˜ ë³€ìˆ˜ â†’ íœ˜ë°œì„± ë©”ëª¨ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598f89d-5a74-4086-92f5-e28c465f1413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_new_env)",
   "language": "python",
   "name": "my_new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
