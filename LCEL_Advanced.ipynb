{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e5b658-879b-4a9e-af25-183b77f1b834",
   "metadata": {},
   "source": [
    "### ğŸ“Œ RunnablePassthrough\n",
    "- ì…ë ¥ì„ ê°€ê³µí•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ëŠ” Runnable\n",
    "- ì£¼ë¡œ **ì²´ì¸ ì¤‘ê°„ ë””ë²„ê¹…/í…ŒìŠ¤íŠ¸** ìš©ë„\n",
    "- íŒŒì´í”„ë¼ì¸ ì„¤ê³„í•  ë•Œ â€œê·¸ëŒ€ë¡œ í˜ë ¤ë³´ë‚´ê¸°â€ ì—­í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6eb901c-aaea-4c01-9b85-a51562d5e758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆë…•í•˜ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "#  RunnablePassthrough\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥\n",
    "passthrough = RunnablePassthrough()\n",
    "print(passthrough.invoke(\"ì•ˆë…•í•˜ì„¸ìš”!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2261c8f9-ba2b-4629-84da-275dca47df49",
   "metadata": {},
   "source": [
    "### ğŸ“Œ Runnable êµ¬ì¡°í™•ì¸\n",
    "- LCELì—ì„œ `|` ì—°ì‚°ìëŠ” **Runnable** ê°ì²´ë¥¼ ì—°ê²°\n",
    "- ëª¨ë“  ì²´ì¸ì€ `RunnableSequence`ë¡œ êµ¬í˜„ë¨\n",
    "- `.input_schema` / `.output_schema` â†’ ì…ë ¥/ì¶œë ¥ íƒ€ì… í™•ì¸ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f97b34d-679f-4815-a560-8168179e2c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-73\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(api_key[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6839a9d-046a-44c2-a566-11253ff77c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "<class 'langchain_core.utils.pydantic.PromptInput'>\n",
      "<class 'langchain_openai.chat_models.base.ChatOpenAIOutput'>\n"
     ]
    }
   ],
   "source": [
    "# Runnable êµ¬ì¡°í™•ì¸\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_template(\"ë„ˆì˜ ì´ë¦„ì€ ë¬´ì—‡ì´ë‹ˆ?\")\n",
    "\n",
    "chain = prompt | llm\n",
    "print(type(chain))          # RunnableSequence\n",
    "print(chain.input_schema)   # ì…ë ¥ êµ¬ì¡°\n",
    "print(chain.output_schema)  # ì¶œë ¥ êµ¬ì¡°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f284044-9050-4f28-befc-3b978d723690",
   "metadata": {},
   "source": [
    "### ğŸ“Œ RunnableLambda\n",
    "- íŒŒì´ì¬ í•¨ìˆ˜ë¥¼ Runnableë¡œ ê°ì‹¸ì„œ ì²´ì¸ì— í¬í•¨ ê°€ëŠ¥\n",
    "- LLM í˜¸ì¶œ ì—†ì´ **ë¡œì§ ì¶”ê°€**í•  ë•Œ ìœ ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "436a2943-3bda-4d83-af91-87ea698aacf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# RunnableLambda\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# ê°„ë‹¨í•œ í•¨ìˆ˜ â†’ Runnableë¡œ ë˜í•‘\n",
    "adder = RunnableLambda(lambda x: x + 10)\n",
    "print(adder.invoke(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e39c6-4858-407c-ab3b-0b416241e723",
   "metadata": {},
   "source": [
    "### ğŸ“Œ Routing\n",
    "- RunnableBranchë¥¼ ì‚¬ìš©í•´ **ì¡°ê±´ì— ë”°ë¼ ì²´ì¸ ë¶„ê¸°**\n",
    "- ì˜ˆ: ìˆ˜í•™ ì§ˆë¬¸ â†’ ê³„ì‚° LLM, ì¼ë°˜ ì§ˆë¬¸ â†’ ëŒ€í™” LLM\n",
    "- ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° ì„¤ê³„ ì‹œ ìœ ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b26b6dd-d82c-4a45-8d21-a2e8d2da8cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='4' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 13, 'total_tokens': 14, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C9vbHxtFCENWwPXUL55zO3Mx0ilKK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--2139ebcb-79b4-40a2-a9e8-ec5bdc5bb499-0' usage_metadata={'input_tokens': 13, 'output_tokens': 1, 'total_tokens': 14, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='ì €ëŠ” í•­ìƒ ê°™ì€ ê¸°ë¶„ì´ì—ìš”. ê·¼ë° ì—¬ëŸ¬ë¶„ì€ ì–´ë– ì‹ ê°€ìš”? í–‰ë³µí•œ í•˜ë£¨ ë˜ì„¸ìš”!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 17, 'total_tokens': 59, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C9vbIrnn6cN9YmeihZVCMeKUKAQe1', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--02e45f63-46e5-4d2d-83d8-7d9435903b83-0' usage_metadata={'input_tokens': 17, 'output_tokens': 42, 'total_tokens': 59, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ì— ë”°ë¼ Routing\n",
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "llm_math = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm_chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "def router(input_text: str):\n",
    "    if any(char.isdigit() for char in input_text):\n",
    "        return \"math\"\n",
    "    return \"chat\"\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: router(x) == \"math\", llm_math),\n",
    "    (lambda x: router(x) == \"chat\", llm_chat),\n",
    "    # default\n",
    "    RunnablePassthrough()\n",
    ")\n",
    "\n",
    "print(branch.invoke(\"2 + 2 = ?\"))\n",
    "print(branch.invoke(\"ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849cab7c-eadd-4520-ae07-f9292c40110a",
   "metadata": {},
   "source": [
    "### ğŸ“Œ RunnableParallel & itemgetter\n",
    "- **RunnableParallel**: ì—¬ëŸ¬ Runnableì„ ë³‘ë ¬ ì‹¤í–‰ â†’ ê²°ê³¼ë¥¼ dictë¡œ ë°˜í™˜\n",
    "- **itemgetter**: ì…ë ¥ dictì—ì„œ íŠ¹ì • key ì¶”ì¶œ\n",
    "- ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ **íš¨ìœ¨ì„± ì¦ê°€** & ë‹¤ì–‘í•œ ì¶œë ¥ ë™ì‹œì— ìƒì„± ê°€ëŠ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e908aac-1ac0-49f3-8bff-84b17cacc60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first': 'ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?', 'second': 'ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?'}\n"
     ]
    }
   ],
   "source": [
    "#  RunnableParallel, itemgetter\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "parallel = RunnableParallel(\n",
    "    first=itemgetter(\"question\"),\n",
    "    second=lambda x: x[\"question\"].upper()\n",
    ")\n",
    "\n",
    "print(parallel.invoke({\"question\": \"ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c608501-0e0c-4a9d-9eec-fe870ebb851d",
   "metadata": {},
   "source": [
    "### ğŸ“Œ Configë¥¼ í™œìš©í•œ ë™ì  ì²´ì¸ ë³€ê²½\n",
    "- `RunnableConfig`ë¡œ ì‹¤í–‰ ì‹œì ì— **LLM, Prompt ë“± êµì²´ ê°€ëŠ¥**\n",
    "- í•˜ë‚˜ì˜ ì²´ì¸ìœ¼ë¡œ ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼/ì„¤ì •ì„ ì‹¤í—˜í•  ìˆ˜ ìˆìŒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60df828c-46e2-48ea-a7f2-7cb5f01b2bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChainì€ ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ì–¸ì–´ ì„œë¹„ìŠ¤ ë° ë²ˆì—­ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” í”Œë«í¼ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì–¸ì–´ ê´€ë ¨ ì„œë¹„ìŠ¤ì˜ íˆ¬ëª…ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë†’ì´ê³ , ì‚¬ìš©ìë“¤ ê°„ì˜ ì‹ ë¢°ë¥¼ ì¦ì§„ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.LangChainì€ ì‚¬ìš©ìë“¤ì´ ì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì–¸ì–´ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 143, 'prompt_tokens': 30, 'total_tokens': 173, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C9vc1RBUAkbAgoW0zJCBkTGtlm3HI', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--2d052cef-fb8e-4a06-84ae-6bf3e0b3a6a6-0' usage_metadata={'input_tokens': 30, 'output_tokens': 143, 'total_tokens': 173, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='LangChainì€ ì–¸ì–´ ë° í†µì—­ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ë¸”ë¡ì²´ì¸ ê¸°ìˆ  ê¸°ë°˜ì˜ í”Œë«í¼ì…ë‹ˆë‹¤. ì´ í”Œë«í¼ì„ í†µí•´ ì‚¬ìš©ìë“¤ì€ ë‹¤ì–‘í•œ ì–¸ì–´ë¡œ í†µì—­ ì„œë¹„ìŠ¤ë¥¼ ë°›ì„ ìˆ˜ ìˆê³ , ë¸”ë¡ì²´ì¸ ê¸°ìˆ ì„ í†µí•´ ì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 119, 'prompt_tokens': 32, 'total_tokens': 151, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C9vc2SxpFSsk9v6AZHru01VjOGvq5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--49c9c493-ec73-415a-8719-8f305606761f-0' usage_metadata={'input_tokens': 32, 'output_tokens': 119, 'total_tokens': 151, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# ë™ì ìœ¼ë¡œ LLM ì´ë‚˜ Prompt ë¥¼ ë³€ê²½í•˜ëŠ” ë°©ë²•(Config)\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"ë„ˆëŠ” ì¹œì ˆí•œ ì–´ì‹œìŠ¤í„´íŠ¸ì•¼. {q}\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"ë„ˆëŠ” ê°„ë‹¨ëª…ë£Œí•œ ì–´ì‹œìŠ¤í„´íŠ¸ì•¼. {q}\")\n",
    "\n",
    "chain = prompt1 | llm\n",
    "\n",
    "# ê¸°ë³¸ ì‹¤í–‰\n",
    "print(chain.invoke({\"q\": \"LangChainì´ ë­ì•¼?\"}))\n",
    "\n",
    "# Configë¡œ prompt ë³€ê²½\n",
    "config = RunnableConfig()\n",
    "dynamic_chain = prompt2 | llm\n",
    "print(dynamic_chain.invoke({\"q\": \"LangChainì´ ë­ì•¼?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a2a9b3-df3d-4745-bc6e-f04ca329a083",
   "metadata": {},
   "source": [
    "### ğŸ“Œ @chain ë°ì½”ë ˆì´í„°\n",
    "- íŒŒì´ì¬ í•¨ìˆ˜ì— `@chain` ë¶™ì´ë©´ Runnableë¡œ ë³€í™˜\n",
    "- `invoke`, `batch`, `stream` ê°™ì€ ë©”ì„œë“œ ì‚¬ìš© ê°€ëŠ¥\n",
    "- ì¥ì :\n",
    "  - ì§ê´€ì ì¸ í•¨ìˆ˜ ì •ì˜\n",
    "  - ê¸°ì¡´ í•¨ìˆ˜ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì²´ì¸ì— í†µí•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7964e78-3644-45c0-bcc9-f7fe119b437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # íŒŒì´ì¬ í•¨ìˆ˜ì— chain ë°ì½”ë ˆì´í„°ë¡œ runnable ì„¤ì •\n",
    "# from langchain.chains import chain\n",
    "\n",
    "# @chain\n",
    "# def custom_chain(x: str) -> str:\n",
    "#     return x[::-1]  # ë¬¸ìì—´ ë’¤ì§‘ê¸°\n",
    "\n",
    "# print(custom_chain.invoke(\"LangChain\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dceb22e-fcc6-4b5c-ad2a-a497dd06e076",
   "metadata": {},
   "source": [
    "# âœ… ìµœì¢… ì •ë¦¬\n",
    "- **RunnablePassthrough** â†’ ì…ë ¥ ê·¸ëŒ€ë¡œ ì „ë‹¬  \n",
    "- **Runnable êµ¬ì¡°í™•ì¸** â†’ ì²´ì¸ input/output schema í™•ì¸  \n",
    "- **RunnableLambda** â†’ í•¨ìˆ˜ ê¸°ë°˜ runnable  \n",
    "- **Routing (Branch)** â†’ ì¡°ê±´ ë¶„ê¸°  \n",
    "- **RunnableParallel & itemgetter** â†’ ë³‘ë ¬ ì‹¤í–‰  \n",
    "- **Config** â†’ ì‹¤í–‰ ì‹œì ì— ë™ì  prompt/LLM ë³€ê²½  \n",
    "- **@chain ë°ì½”ë ˆì´í„°** â†’ íŒŒì´ì¬ í•¨ìˆ˜ë„ runnable ì²´ì¸ìœ¼ë¡œ í™•ì¥  \n",
    "\n",
    "ğŸ‘‰ LCEL ê³ ê¸‰ ë¬¸ë²•ì€ RAG, Agent, Workflow êµ¬í˜„ ì‹œ **ìœ ì—°í•œ ì²´ì¸ êµ¬ì„±**ì„ ê°€ëŠ¥í•˜ê²Œ í•´ì¤Œ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_new_env)",
   "language": "python",
   "name": "my_new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
